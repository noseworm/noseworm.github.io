<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Michael Noseworthy</title>

    <meta name="author" content="Michael Noseworthy">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Michael Noseworthy
                </p>
                <p>I am a graduate student in the <a href="https://groups.csail.mit.edu/rrg/">Robust Robotics Group</a> at MIT.
                </p>
                <p>
                  My research focuses on robust planning under uncertainty motivated by long-horizon manipulation tasks such as assembly or rearrangement. I am also a part-time intern at the <a href="https://research.nvidia.com/labs/srl/">NVIDIA Seattle Robotics Lab</a> where I work on contact-rich manipulation. Before MIT, I studied dialogue systems at McGill University's <a href="https://rl.cs.mcgill.ca/"> Reasoning and Learning Lab</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:mnosew@mit.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Ybj21gEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/noseworm">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/noseworm/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/headshot2.jpg">
                  <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/headshot2.jpg" class="hoverZoomLink">
                </a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  TODO: Add research statement.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <!-- ICRA 2024 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/icra24.jpg" alt="Panda Grasping" height="160" style="border-style: none">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Amortized Inference for Efficient Grasp Model Adaptation
        </span>
        <br>
		      <strong>Michael Noseworthy*</strong>,
		      <span>Seiji Shaw*</span>,
		      <span>Chad Kessens</span>,
	      	<span="">Nicholas Roy</span>
        <br>
          ICRA 2024
        <br>
        <p></p>
        <p>
          Adaptively grasping objects without unknown dynamics properties (e.g., mass distribution or frictional coefficients).
        </p>
        <a href="http://groups.csail.mit.edu/rrg/papers/noseworthy_shaw_icra24.pdf">Paper</a>
      </td>
    </tr>
    
    <!-- NeurIPS: Robot Learning Workshop, 2022 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
       <video  width=100% muted autoplay loop>
          <source src="images/asymm_rnn_zoom.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Insights towards Sim2Real Contact-Rich Manipulation
        </span>
        <br>
		      <strong>Michael Noseworthy</strong>,
		      <span>Iretiayo Akinola</span>,
		      <span>Yashraj Narang</span>,
	      	<span>Fabio Ramos</span>,
          <span>Lucas Manuelli</span>,
          <span>Ankur Handa</span>,
          <span>Dieter Fox</span>
        <br>
          NeurIPS 2022: Robot Learning Workshop
        <br>
        <p></p>
        <p>
          Training policies to solve contact-rich manipulation tasks with noisy pose estimates.
        </p>
        <a href="http://www.robot-learning.ml/2022/files/19.zip">Paper</a>
      </td>
    </tr>

    <!-- NeurIPS: Bayesian Deep Learning, 2021 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/bdl21.png" alt="Learning Phases" height="120" style="border-style: solid; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Object-Factored Models with Partially Observable State
        </span>
        <br>
          <span>Isaiah Brand*</span>,
		      <strong>Michael Noseworthy*</strong>,   
		      <span>Sebastian Castro</span>,
	      	<span="">Nicholas Roy</span>
        <br>
          NeurIPS 2021: Bayesian Deep Learning Workshop
        <br>
        <p></p>
        <p>
          Efficient adaptation for manipulating objects with non-visual parameters.
        </p>
        <a href="http://bayesiandeeplearning.org/2021/papers/53.pdf">Paper</a>
      </td>
    </tr>

    <!-- RSS 2021-->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
       <video  width=100% muted autoplay loop>
          <source src="images/training_sample.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Active Learning of Abstract Plan Feasibility
        </span>
        <br>
		      <strong>Michael Noseworthy*</strong>,
		      <span>Caris Moses*</span>,
		      <span>Isaiah Brand*</span>,
	      	<span>Sebastian Castro</span>,
          <span>Leslie Kaelbling</span>,
          <span>Tom&aacute;s Lozano-P&eacute;rez</span>,
          <span>Nicholas Roy</span>
        <br>
          RSS 2021
        <br>
        <p></p>
        <p>
          Efficient online learning of feasility models using ensembles of graph networks.
        </p>
        <a href="https://www.roboticsproceedings.org/rss17/p043.pdf">Paper</a>
        /
        <a href="https://www.youtube.com/watch?v=NM3sv6hzx90">Talk</a>
      </td>
    </tr>

    <!-- ICRA 2020-->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
       <video  width=100% muted autoplay loop>
          <source src="images/baxter_prior_fast.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Visual Prediction of Priors for Articulated Object Interaction
        </span>
        <br>
          <span>Caris Moses*</span>,
		      <strong>Michael Noseworthy*</strong>,
          <span>Leslie Kaelbling</span>,
          <span>Tom&aacute;s Lozano-P&eacute;rez</span>,
          <span>Nicholas Roy</span>
        <br>
          ICRA 2020
        <br>
        <p></p>
        <p>
          Efficient manipulation of articulated objects using visual priors to infer kinematic parameters.
        </p>
        <a href="http://people.csail.mit.edu/cmm386/publications/vis_prediction_priors.pdf">Paper</a>
        /
        <a href="https://www.youtube.com/watch?v=6ABBn1KpSSU">Talk</a>
        /
        <a href="https://github.com/robustrobotics/honda_cmm">Code</a>
        /
        <a href="https://sites.google.com/view/contextual-prior-prediction">Website</a>
      </td>
    </tr>

    <!-- CORL 2019 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/corl19.png" alt="Pour Action Demonstration" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Task-Conditioned Variational Autoencoders for Learning Movement Primitives
        </span>
        <br>
		      <strong>Michael Noseworthy</strong>,   
		      <span>Rohan Paul</span>,
          <span>Subhro Roy</span>,
          <span>Daehyung Park</span>,
	      	<span="">Nicholas Roy</span>
        <br>
          CORL 2019
        <br>
        <p></p>
        <p>
          Learning interpretable movement primitives from demonstration.
        </p>
        <a href="https://proceedings.mlr.press/v100/noseworthy20a.html">Paper</a>
      </td>
    </tr>

    <!-- CORL 2019 - Daehyung -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/corl19_park.png" alt="Robot Pipe" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning
        </span>
        <br>
          <span>Daehyung Park</span>,
		      <strong>Michael Noseworthy</strong>,   
		      <span>Rohan Paul</span>,
          <span>Subhro Roy</span>,
	      	<span="">Nicholas Roy</span>
        <br>
          CORL 2019
        <br>
        <p></p>
        <p>
          Learning from demonstration in the presence of complex constraints.
        </p>
        <a href="https://proceedings.mlr.press/v100/park20a.html">Paper</a>
      </td>
    </tr>

    <!-- CONLL 2019 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/conll19.png" alt="Interaction example" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Leveraging Past References for Robust Language Grounding
        </span>
        <br>
          <span>Subhro Roy*</span>,
          <strong>Michael Noseworthy*</strong>,
          <span>Rohan Paul</span>,
          <span>Daehyung Park</span>,
	      	<span="">Nicholas Roy</span>
        <br>
        CoNLL 2019
        <br>
        <p></p>
        <p>
          Natural language grounding in situated and temporally extended contexts.
        </p>
        <a href="https://aclanthology.org/K19-1040/">Paper</a>
      </td>
    </tr>

     <!-- ACL 2017 -->
     <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/adem.png" alt="Score correlation" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses
        </span>
        <br>
          <span>Ryan Lowe*</span>,
          <strong>Michael Noseworthy*</strong>,
          <span>Iulian Vlad Serban</span>,
          <span>Nicolas Angelard-Gontier</span>,
	      	<span="">Yoshua Bengio</span>,
          <span="">Joelle Pineau</span>
        <br>
        ACL 2017
        <br>
        <p></p>
        <p>
          Automatic metric for dialogue model response evaluation.
        </p>
        <a href="https://aclanthology.org/P17-1103.pdf">Paper</a>
        /
        <a href="https://github.com/noseworm/ADEM">Code</a>
        /
        <a href="https://vimeo.com/234958888">Talk</a>
      </td>
    </tr>

    <!-- SIGDIAL 2017 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/sigdial17.png" alt="TSNE Plot" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Predicting Success in Goal-Driven Human-Human Dialogues
        </span>
        <br>
          <strong>Michael Noseworthy</strong>,
	      	<span="">Jackie Chi Kit Cheung</span>,
          <span="">Joelle Pineau</span>
        <br>
        SIGDIAL 2017
        <br>
        <p></p>
        <p>
          Automatic success prediction for task-driven dialogue systems.
        </p>
        <a href="https://aclanthology.org/W17-5531/">Paper</a>
      </td>
    </tr>

    <!-- EMNLP 2016 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/emnlp16.png" alt="Score correlation" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
        </span>
        <br>
          <span>Chia-Wei Liu*</span>,
          <span>Ryan Lowe*</span>,
          <span>Iulian Vlad Serban*</span>,
          <strong>Michael Noseworthy*</strong>,
	      	<span="">Laurent Charlin</span>,
          <span="">Joelle Pineau</span>
        <br>
        EMNLP 2017
        <br>
        <p></p>
        <p>
          A study of how common automatic metrics for evaluating dialogue responses correlate with human judgement.
        </p>
        <a href="https://aclanthology.org/D16-1230/">Paper</a>
        /
        <a href="https://vimeo.com/239251122">Talk</a>
      </td>
    </tr>

<!--
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/icra24.jpg" alt="icra24" height="160" style="border-style: none">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">Amortized Inference for Efficient Grasp Model Adaptation</span>
        <br>
		<strong>Michael Noseworthy*</strong>,
		<span>Seiji Shaw*</span>,
		<span>Chead Kessens</span>,
		<span="">Nicholas Roy</span>,
        <br>
        To appear at <em>ICRA</em>, 2024
        <br>
        <a href="https://smerf-3d.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
      </td>
    </tr>
  -->

    
     

      

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;"><img width="100" src="images/qinai_small.png"></td>
              <td width="75%" valign="center">
                <a href="https://sites.google.com/robot-learning.org/corl2020/attending/inclusioncorl">Inclusion@CoRL Organizer, CoRL 2020</a>
                <br>
                Queer in AI Organizer, RSS 2021
                <br>
                <a href="https://sites.google.com/view/queer-in-ai/corl-2021?authuser=0">Queer in AI Organizer, CoRL 2021</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:xsmall;">
                  Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
